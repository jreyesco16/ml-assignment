{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csce478    Programming Assignment 4     Fall 2020                                   Due: Nov 17 11am\n",
    "# Linear Support Vector Machine & Principle Component Analysis\n",
    "# Contributers: Chungsum Kim, Devn Steiner, Jesse Reyes Cortes\n",
    "\n",
    "#imported packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import copy\n",
    "import itertools\n",
    "from random import sample \n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from random import seed\n",
    "seed(1)\n",
    "from random import random\n",
    "\n",
    "# helper functions\n",
    "def accuracy(x,y):\n",
    "    count = 0\n",
    "    for i in range(0, len(x)):\n",
    "        if(x[i]==0 and 0 == y[i]):\n",
    "            count = count + 1\n",
    "        elif(x[i]==1 and 0 == y[i]):\n",
    "            count = count + 1\n",
    "        elif(x[i]==2 and y[i]==2):\n",
    "            count = count + 1\n",
    "    \n",
    "    return count/len(x)\n",
    "\n",
    "\n",
    "def confusion_matrix(x,y):\n",
    "    TN = 0\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "    TP = 0\n",
    "    for i in range(0, len(y)):\n",
    "        if(x[i]==0 and y[i] <= .5):\n",
    "            TN = TN + 1\n",
    "        elif(x[i]==0 and .5 < y[i]):\n",
    "            FP = FP + 1\n",
    "        elif(x[i]==1 and y[i] <= .5):\n",
    "            FN = FN + 1\n",
    "        elif(x[i]==1 and .5 < y[i]):\n",
    "            TP = TP + 1\n",
    "\n",
    "    return [[TN, FP],[FN, TP]]\n",
    "\n",
    "\n",
    "def adaptive_learning_rate(t_0, t_1, iteration):\n",
    "    return t_0/(iteration + t_1)\n",
    "\n",
    "\n",
    "def kFoldHelper(X,Y, k):\n",
    "    \n",
    "    kData = []\n",
    "    \n",
    "    x = np.array_split(X,k)\n",
    "    y = np.array_split(Y,k)\n",
    "    \n",
    "    for i in range(0, k):\n",
    "        x_train = np.array([])\n",
    "        y_train = np.array([])\n",
    "        x_test = x[i]\n",
    "        y_test = y[i]\n",
    "        for j in range(0,k):\n",
    "            if i != j:\n",
    "                if(len(x_train)==0):\n",
    "                    x_train = x[j]\n",
    "                else:\n",
    "                    x_train = np.concatenate((x_train,x[j]))\n",
    "                y_train = np.concatenate((y_train, y[j]))\n",
    "            \n",
    "        kData.append([x_train,y_train,x_test,y_test])\n",
    "    \n",
    "    return kData\n",
    "\n",
    "\n",
    "def t_sv(X,w,b):\n",
    "        \n",
    "    t = np.array([])\n",
    "        \n",
    "    for j in range(0, len(X)):\n",
    "        if np.dot(np.transpose(w), X[j])+b >= 1 :\n",
    "            t = np.append(t, [1])\n",
    "        else:\n",
    "            t = np.append(t, [0])\n",
    "                \n",
    "    return t\n",
    "\n",
    "\n",
    "def cost_function_svm(C,w,b, t, x_sv):\n",
    "    \n",
    "    wt = w.reshape((1,2))\n",
    "    \n",
    "    x_sv = x_sv.reshape(120,1)\n",
    "    \n",
    "        \n",
    "    j = (1/2) * np.matmul(np.transpose(w), w) + C * ((np.sum(1-np.matmul(x_sv, wt))) - b * np.sum(t))\n",
    "        \n",
    "    return j\n",
    "\n",
    "\n",
    "def decision_boundary_support_vectors(svm_clf, X):\n",
    "    \n",
    "    xmin, xmax = X.min() - 1, X.max() + 1\n",
    "    \n",
    "    w = svm_clf.coef_[0]\n",
    "    b = svm_clf.intercept_[0]\n",
    "\n",
    "    # At the decision boundary, w1*x1 + w2*x2 + b = 0\n",
    "    # => x2 = -(b + w1* x1)/w1\n",
    "    x1 = np.linspace(xmin, xmax, 100)\n",
    "    \n",
    "    decision_boundary = -(b + w[0]*x1)/w[1]\n",
    "\n",
    "    shifting_factor_for_margin = 1/w[1]\n",
    "    upper_margin = decision_boundary + shifting_factor_for_margin\n",
    "    lower_margin = decision_boundary - shifting_factor_for_margin\n",
    "\n",
    "    svs = svm_clf.support_vectors_\n",
    "    plt.scatter(svs[:, 0], svs[:, 1], s=200, facecolors='g', label=\"Support Vectors\")\n",
    "    plt.plot(x1, decision_boundary, \"k-\", linewidth=2)\n",
    "    plt.plot(x1, upper_margin, \"k--\", linewidth=2)\n",
    "    plt.plot(x1, lower_margin, \"k--\", linewidth=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part A : Linear Support Vector Machine (SVM)\n",
    "\n",
    "# A.1 implement Linear_SVC model class for performing binary classification\n",
    "\n",
    "class Linear_SVC:\n",
    "    def __init__(self, C=1, max_iter=100, tol=None, learning_rate=\"constant\",learning_rate_init=0.001, t_0=1, t_1=1000, early_stopping=False, validation_fraction=0.1, **kwargs):\n",
    "        self.C = C\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.learning_rate = learning_rate\n",
    "        self.learning_rate_init = learning_rate_init\n",
    "        self.t_0 = t_0\n",
    "        self.t_1 = t_1\n",
    "        self.early_stopping = early_stopping\n",
    "        self.validation_fraction = validation_fraction\n",
    "        \n",
    "        \n",
    "    def fit(self, X, Y):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        \n",
    "        # calculate the weight vector w with length of one sample with random small number\n",
    "        self.w = np.array([])\n",
    "        for i in range(0, len(self.X[0])):\n",
    "            self.w = np.append(self.w, [random()])\n",
    "            \n",
    "        print(self.w)\n",
    "        \n",
    "        # intercept bias b originally set to 0\n",
    "        self.b = 0\n",
    "        \n",
    "        \n",
    "        \n",
    "        for i in range(0, self.max_iter):\n",
    "            \n",
    "            t = t_sv(self.X, self.w, self.b)\n",
    "            \n",
    "            x_sv = t * (np.matmul(self.X, self.w) + self.b)\n",
    "            \n",
    "            # early stopping\n",
    "            if abs(cost_function_svm(self.C, self.w, self.b, t, x_sv)) < self.tol:\n",
    "                break\n",
    "                \n",
    "            \n",
    "            grad_w = self.w - self.C * np.sum(x_sv)\n",
    "            grad_b = -self.C * np.sum(t)\n",
    "            \n",
    "            self.w = self.w - grad_w\n",
    "            self.b = self.b - grad_b\n",
    "        \n",
    "        self.intercept = np.array([self.b]) \n",
    "        self.coef = np.array([self.w]) \n",
    "        self.support_vectors = np.array([x_sv])\n",
    "        \n",
    "        return self\n",
    "            \n",
    "        \n",
    "    def predict(self, X):\n",
    "        \n",
    "        print(X)\n",
    "        \n",
    "        \n",
    "        return self.Y\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary Classification using Linear_SVC Classifier\n",
    "\n",
    "# A.2 read the iris data, X features: petal length & petal width, recode binary target such that Iris-Virginica is 1 or 0\n",
    "data = load_iris()\n",
    "\n",
    "pl = data[\"data\"][:,2]\n",
    "pw = data[\"data\"][:,3]\n",
    "X = np.zeros((len(pl), 2))\n",
    "X[:,0] = pl\n",
    "X[:,1] = pw\n",
    "\n",
    "Y = np.where(data['target'] == 2, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A.3 partition data into train & test set (80% - 20%)\n",
    "def partition(X, Y, t):     \n",
    "    test_size = math.ceil(len(Y)*t)\n",
    "    training_size = len(Y)-test_size\n",
    "    training_data = X[0:training_size][:]\n",
    "    testing_data = X[training_size+1:len(Y)] [:]\n",
    "    training_vector = Y[0:training_size]\n",
    "    testing_vector = Y[training_size+1:len(Y)]\n",
    "    \n",
    "    return training_data, testing_data, training_vector, testing_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searning . . .\n",
      "[0.13436424 0.84743374]\n",
      "[[1.4 0.2]\n",
      " [1.4 0.2]\n",
      " [1.3 0.2]\n",
      " [1.5 0.2]\n",
      " [1.4 0.2]\n",
      " [1.7 0.4]\n",
      " [1.4 0.3]\n",
      " [1.5 0.2]\n",
      " [1.4 0.2]\n",
      " [1.5 0.1]\n",
      " [1.5 0.2]\n",
      " [1.6 0.2]\n",
      " [1.4 0.1]\n",
      " [1.1 0.1]\n",
      " [1.2 0.2]\n",
      " [1.5 0.4]\n",
      " [1.3 0.4]\n",
      " [1.4 0.3]\n",
      " [1.7 0.3]\n",
      " [1.5 0.3]\n",
      " [1.7 0.2]\n",
      " [1.5 0.4]\n",
      " [1.  0.2]\n",
      " [1.7 0.5]\n",
      " [1.9 0.2]\n",
      " [1.6 0.2]\n",
      " [1.6 0.4]\n",
      " [1.5 0.2]\n",
      " [1.4 0.2]\n",
      " [1.6 0.2]]\n",
      "Searning . . .\n",
      "[0.76377462 0.25506903]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-193-09d065fcadcf>:100: RuntimeWarning: overflow encountered in matmul\n",
      "  j = (1/2) * np.matmul(np.transpose(w), w) + C * ((np.sum(1-np.matmul(x_sv, wt))) - b * np.sum(t))\n",
      "<ipython-input-193-09d065fcadcf>:100: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  j = (1/2) * np.matmul(np.transpose(w), w) + C * ((np.sum(1-np.matmul(x_sv, wt))) - b * np.sum(t))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.6 0.2]\n",
      " [1.5 0.4]\n",
      " [1.5 0.1]\n",
      " [1.4 0.2]\n",
      " [1.5 0.2]\n",
      " [1.2 0.2]\n",
      " [1.3 0.2]\n",
      " [1.4 0.1]\n",
      " [1.3 0.2]\n",
      " [1.5 0.2]\n",
      " [1.3 0.3]\n",
      " [1.3 0.3]\n",
      " [1.3 0.2]\n",
      " [1.6 0.6]\n",
      " [1.9 0.4]\n",
      " [1.4 0.3]\n",
      " [1.6 0.2]\n",
      " [1.4 0.2]\n",
      " [1.5 0.2]\n",
      " [1.4 0.2]\n",
      " [4.7 1.4]\n",
      " [4.5 1.5]\n",
      " [4.9 1.5]\n",
      " [4.  1.3]\n",
      " [4.6 1.5]\n",
      " [4.5 1.3]\n",
      " [4.7 1.6]\n",
      " [3.3 1. ]\n",
      " [4.6 1.3]\n",
      " [3.9 1.4]]\n",
      "Searning . . .\n",
      "[0.49543509 0.44949106]\n",
      "[[3.5 1. ]\n",
      " [4.2 1.5]\n",
      " [4.  1. ]\n",
      " [4.7 1.4]\n",
      " [3.6 1.3]\n",
      " [4.4 1.4]\n",
      " [4.5 1.5]\n",
      " [4.1 1. ]\n",
      " [4.5 1.5]\n",
      " [3.9 1.1]\n",
      " [4.8 1.8]\n",
      " [4.  1.3]\n",
      " [4.9 1.5]\n",
      " [4.7 1.2]\n",
      " [4.3 1.3]\n",
      " [4.4 1.4]\n",
      " [4.8 1.4]\n",
      " [5.  1.7]\n",
      " [4.5 1.5]\n",
      " [3.5 1. ]\n",
      " [3.8 1.1]\n",
      " [3.7 1. ]\n",
      " [3.9 1.2]\n",
      " [5.1 1.6]\n",
      " [4.5 1.5]\n",
      " [4.5 1.6]\n",
      " [4.7 1.5]\n",
      " [4.4 1.3]\n",
      " [4.1 1.3]\n",
      " [4.  1.3]]\n",
      "Searning . . .\n",
      "[0.65159297 0.78872335]\n",
      "[[4.4 1.2]\n",
      " [4.6 1.4]\n",
      " [4.  1.2]\n",
      " [3.3 1. ]\n",
      " [4.2 1.3]\n",
      " [4.2 1.2]\n",
      " [4.2 1.3]\n",
      " [4.3 1.3]\n",
      " [3.  1.1]\n",
      " [4.1 1.3]\n",
      " [6.  2.5]\n",
      " [5.1 1.9]\n",
      " [5.9 2.1]\n",
      " [5.6 1.8]\n",
      " [5.8 2.2]\n",
      " [6.6 2.1]\n",
      " [4.5 1.7]\n",
      " [6.3 1.8]\n",
      " [5.8 1.8]\n",
      " [6.1 2.5]\n",
      " [5.1 2. ]\n",
      " [5.3 1.9]\n",
      " [5.5 2.1]\n",
      " [5.  2. ]\n",
      " [5.1 2.4]\n",
      " [5.3 2.3]\n",
      " [5.5 1.8]\n",
      " [6.7 2.2]\n",
      " [6.9 2.3]\n",
      " [5.  1.5]]\n",
      "Searning . . .\n",
      "[0.09385959 0.02834748]\n",
      "[[5.7 2.3]\n",
      " [4.9 2. ]\n",
      " [6.7 2. ]\n",
      " [4.9 1.8]\n",
      " [5.7 2.1]\n",
      " [6.  1.8]\n",
      " [4.8 1.8]\n",
      " [4.9 1.8]\n",
      " [5.6 2.1]\n",
      " [5.8 1.6]\n",
      " [6.1 1.9]\n",
      " [6.4 2. ]\n",
      " [5.6 2.2]\n",
      " [5.1 1.5]\n",
      " [5.6 1.4]\n",
      " [6.1 2.3]\n",
      " [5.6 2.4]\n",
      " [5.5 1.8]\n",
      " [4.8 1.8]\n",
      " [5.4 2.1]\n",
      " [5.6 2.4]\n",
      " [5.1 2.3]\n",
      " [5.1 1.9]\n",
      " [5.9 2.3]\n",
      " [5.7 2.5]\n",
      " [5.2 2.3]\n",
      " [5.  1.9]\n",
      " [5.2 2. ]\n",
      " [5.4 2.3]\n",
      " [5.1 1.8]]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 150 is out of bounds for axis 0 with size 150",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-197-5ec0446e69f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m                     \u001b[0mdict_svm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"learn rate\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m                     \u001b[0mdict_svm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_hat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m                     \u001b[0mdict_svm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"confusion matrix\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_hat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-193-09d065fcadcf>\u001b[0m in \u001b[0;36mconfusion_matrix\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mTP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m             \u001b[0mTN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTN\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32melif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;36m.5\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 150 is out of bounds for axis 0 with size 150"
     ]
    }
   ],
   "source": [
    "# A.4 Hyper-parameter training(kFold - C, learning_rate, max_iter, tol)\n",
    "\n",
    "#testing params\n",
    "Cs = [.1,1]\n",
    "learning_rate_inits = [.001, .1]\n",
    "max_iters = [100, 1000]\n",
    "tol = [.0001, .01]\n",
    "learning_rates = [\"constant\", \"adaptive\"]\n",
    "\n",
    "data = kFoldHelper(X, Y, 5)\n",
    "\n",
    "dict_svm = {\"C\":[], \"learn rate init\":[], \"max iter\":[], \"tol\":[], \"learn rate\":[], \"accuracy\":[], \"confusion matrix\":[]}\n",
    "\n",
    "for c in Cs:\n",
    "    for lri in learning_rate_inits:\n",
    "        for mi in max_iters:\n",
    "            for t in tol:\n",
    "                for lr in learning_rates:\n",
    "                    y_hat = np.array([])\n",
    "                    for d in data:\n",
    "                        (print(\"Searning . . .\"))\n",
    "                        \n",
    "                        x_train = d[0]\n",
    "                        y_train = d[1]\n",
    "                        x_test = d[2]\n",
    "                        y_test = d[3]\n",
    "                                                \n",
    "                        model = Linear_SVC(C=c,max_iter=mi,tol=t,learning_rate=lr,learning_rate_init=lri,early_stopping=True)\n",
    "                        \n",
    "                        model.fit(x_train,y_train)\n",
    "                        \n",
    "                        y_hat = np.append(y_hat, model.predict(x_test))\n",
    "                        \n",
    "                        \n",
    "                    #update dict\n",
    "                    dict_svm[\"C\"].append(c)\n",
    "                    dict_svm[\"learn rate init\"].append(lri)\n",
    "                    dict_svm[\"max iter\"].append(mi)\n",
    "                    dict_svm[\"tol\"].append(t)\n",
    "                    dict_svm[\"learn rate\"].append(lr)\n",
    "                    dict_svm[\"accuracy\"].append(accuracy(Y,y_hat))\n",
    "                    dict_svm[\"confusion matrix\"].append(confusion_matrix(Y,y_hat))\n",
    "                        \n",
    "                        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [C, learn rate init, max iter, tol, learn rate, accuracy, confusion matrix]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# A.5 train model using optimal values for hyperparameters on test data, report accuracy and test confusion matrix\n",
    "print(pd.DataFrame(dict_svm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'drop'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-99-8415b9017d78>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# A.6 plot the learning curve NOT COMPLETED\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'quality'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m## standardized data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'quality'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLinear_Regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'drop'"
     ]
    }
   ],
   "source": [
    "# A.6 plot the learning curve NOT COMPLETED\n",
    "X = data.drop(columns='quality')\n",
    "X = (X - X.mean())/X.std() ## standardized data\n",
    "Y = data['quality']\n",
    "model = Linear_Regression()\n",
    "model.fit(X, Y, learning_rate=.01,lambd=1, regularizer='l2')\n",
    "model.plot_curve()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'coef_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-100-471080098592>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0msvm_clf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mdecision_boundary_support_vectors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msvm_clf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-93-7015a0cc5a7a>\u001b[0m in \u001b[0;36mdecision_boundary_support_vectors\u001b[0;34m(svm_clf, X)\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[0mxmin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxmax\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m     \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvm_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m     \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvm_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintercept_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'coef_'"
     ]
    }
   ],
   "source": [
    "# A.7 plot the decision boundary and show support vectors using “decision_boundary_support_vectors”\n",
    "# https://github.com/rhasanbd/Support-Vector-Machine-Classifier-Beginners-Survival-Kit/blob/master/Support%20Vector%20Machine-1-Linearly%20Separable%20Data.ipynb\n",
    "# Draw a scatter plot\n",
    "\n",
    "# need to figure out what \n",
    "svm_clf = []\n",
    "\n",
    "decision_boundary_support_vectors(svm_clf, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A.8 implement early stopping in the \"fit\" method of the Linear_SVC model, generate early stopping code\n",
    "\n",
    "\n",
    "# this implementation was integrated into the Linear_SVM model A.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part B: Principle Component Analysis\n",
    "\n",
    "# B.9 using matplotlib.pyploy \"imread\" function read the image as a 2D matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B.10 implement steps of eigendecomposition based PCA on X\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B.11 find top k eigenvectors and create eigen vector matrix using top k eigenvectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B.12 finally project mean centered data on the k top eigenvectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B.13 reconstruct data matrix by taking dot product between projected data and transpose of top K eigenvec matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B.14 compute reconstruction error between mean centered data matrix X and reconstructed data matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B.15 perform steps 11-14 for k: 10, 30, 50, 100, 500\n",
    "# for each k reconstruct image & print the value of k and reconstruction error"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
