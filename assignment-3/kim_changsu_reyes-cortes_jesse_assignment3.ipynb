{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
<<<<<<< HEAD
=======
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.791182780644077e-06\n",
      "1.6890237283354335e-05\n",
      "9.082877850236406e-05\n",
      "4.0787816547122535e-05\n",
      "2.1447173200085966e-08\n",
      "6.327340772053627e-07\n",
      "9.082877850236406e-05\n",
      "4.0787816547122535e-05\n",
      "1.903079168620961e-05\n",
      "1.617785992854619e-05\n",
      "0.0005630941383019991\n",
      "0.0038333522985889846\n",
      "[0, 1, 0, 1, 1, 0]\n"
     ]
    }
   ],
>>>>>>> parent of d6a8814... Merge pull request #9 from jreyesco16/devan
   "source": [
    "# csce 487 intro to ML\n",
    "# assignment 3 - Naive Bays Classifier & Logistic Regression\n",
    "# collaborators : Changsu Kim, Jesse Reyes Cortes\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "from random import sample \n",
    "import nltk\n",
    "\n",
    "\n",
    "#################             NAIVE BAYES CLASSIFIER             #################\n",
    "\n",
    "# part A: model code (design multinomial naive bayes)\n",
    "\n",
    "data_A = [[2, 3, 2, 2, 3, 2, 2],\n",
    "          [22, 22, 33, 2 , 2, 2],\n",
    "          [3, 2, 4, 3, 8, 4],\n",
    "          [2, 2, 3,2, 2, 2,2, 2, 2, 3],\n",
    "          [2,3,2,4, 7,6,5,5,4],\n",
    "          [2, 3, 9, 4, 4, 29, 29, 2],\n",
    "          [2,9, 7, 8 , 23, 43, 29, 28],\n",
    "          [2, 43,2, 4,3, 5, 4, 9, 20, 24],\n",
    "          [2, 4 ,3 ,2, 3, 2],\n",
    "          [2, 4, 8, 8, 4, 2,3,53],\n",
    "          [4, 34, 23, 23, 23,2],\n",
    "          [2, 3,3,2, 3,4, 5,3, 4],\n",
    "          [2, 4, 2, 3, 2, 4, 5],\n",
    "          [22,43,23,23,43,34,23,23,5],\n",
    "          [23, 8, 2, 2, 3, 4]\n",
    "         ]\n",
    "label_A = [1,0,1,0,1,1,0,1,0,1,1,0,0,1,0]\n",
    "\n",
    "test_A = [[2,2, 23, 23, 2, 2, 2],\n",
    "          [2, 3, 2, 3, 2, 3],\n",
    "          [4,5,4,3,3,2,43,32],\n",
    "          [3,2,3,2,2,3],\n",
    "          [2,3,2,3,2,4,2,42],\n",
    "          [23,4,2,32]]\n",
    "\n",
    "test_A_labels = [0,0,1,0,1,0]\n",
    "\n",
    "class Multinomial_NB:\n",
    "    def __init__(self, alpha=1.0):\n",
    "        self.X = np.array([])\n",
    "        self.Y = np.array([])\n",
    "        self.alpha = alpha\n",
    "        self.prob = np.array([])\n",
    "        self.s_count = 0\n",
    "        self.s_count_total = 0\n",
    "        self.h_count =0\n",
    "        self.h_count_total = 0\n",
    "        self.hashtable = {\"spam\" : {}, \"ham\" : {}}\n",
    "        \n",
    "        \n",
    "    def fit(self, X, Y):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        \n",
    "        for i in range(0, len(self.X)):\n",
    "            \n",
    "            c = \"spam\"\n",
    "            \n",
    "            # spam\n",
    "            if(self.Y[i]==1):\n",
    "                self.s_count = self.s_count + 1\n",
    "                self.s_count_total = self.s_count_total + len(self.X[i])\n",
    "                c = \"ham\"\n",
    "            # not spam\n",
    "            else:\n",
    "                self.h_count = self.h_count + 1\n",
    "                self.h_count_total = self.h_count_total + len(self.X[i])\n",
    "                \n",
    "            for j in self.X[i]:\n",
    "                #if it j feature is already in hashtable\n",
    "                if str(j) in self.hashtable[c]:\n",
    "                    self.hashtable[c][str(j)] = self.hashtable[c][str(j)] + 1\n",
    "                else:\n",
    "                    self.hashtable[c][str(j)] = 1\n",
    "                    \n",
    "        print(self.s_count_total)\n",
    "        print(self.h_count_total)\n",
    "        print(self.hashtable)    \n",
    "        \n",
    "        \n",
    "    def predict(self, X):\n",
    "        \n",
    "        predict = []\n",
    "        \n",
    "        pie_s = self.s_count/(self.s_count + self.h_count)\n",
    "        pie_h = self.h_count/(self.s_count + self.h_count)\n",
    "        \n",
    "        for i in range(0, len(X)):\n",
    "            \n",
    "            #predicting spam\n",
    "            p_s = 1\n",
    "            #predicting ham\n",
    "            p_h = 1\n",
    "            \n",
    "            for j in X[i]:\n",
    "                if str(j) in self.hashtable[\"spam\"]:\n",
    "                    p_s = p_s * ((self.hashtable[\"spam\"][str(j)])/self.s_count_total)\n",
    "                    \n",
    "                if str(j) in self.hashtable[\"ham\"]:\n",
    "                    p_h = p_h * ((self.hashtable[\"ham\"][str(j)])/self.h_count_total)\n",
    "                    \n",
    "            p_s = pie_s * p_s\n",
    "            \n",
    "            p_h = pie_h * p_h\n",
    "                \n",
    "            if(p_s > p_h):\n",
    "                predict.append(1)\n",
    "                \n",
    "            else:\n",
    "                predict.append(0)\n",
    "                \n",
    "        return predict\n",
    "    \n",
    "    def predict_log_proba(self, X):\n",
    "        print(X)\n",
    "        \n",
    "        \n",
    "model = Multinomial_NB()\n",
    "model.fit(data_A, label_A)\n",
    "Y_hat = model.predict(test_A)\n",
    "print(Y_hat)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
=======
   "execution_count": 43,
>>>>>>> parent of d6a8814... Merge pull request #9 from jreyesco16/devan
   "metadata": {},
   "outputs": [],
   "source": [
    "# part B: exploratory data analysis\n",
    "# B.2 read in SMSSpamCollection.csv as DataFrame\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B.3 summerization of data in terms of mean, standard deviation, and quartiles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B.4 generate plot to display class distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part C: Feature Extraction\n",
    "# C.5 normalize text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C.6 generate word clouds for both spam and ham emails (NLTK lib)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C.7 remove stop words & convert to numerical feature vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C.8 create data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C.9 shuffle rows of data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C.10 partition data (80% train - 20% test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part D: Model Evaluation\n",
    "# D.11 model selection via hyper-parameter tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D.12 generate ROC and AUC (bonus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D.13 evaluate model on test day (Precision, Recall, F1 score, Confusion matrix, Accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D.14 multivariate bernoulli naive bayes model (bonus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################      LOGISTIC REGRESSION: MULTI-CLASS CLASSIFICATION      #################\n",
    "# part A: model code\n",
    "# A.15 design softmax  regression classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A.16 compute softmax score \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A.17 implement function to compute the cross-entropy loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A.18 implement a softmax_regression model class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B.19 read iris data (use sklearn.datasets.load_iris)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B.20 summerize variables in terms of mean, standard deviation, and quartiles (use technique from 2nd recitation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B.21 shuffle rows of data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B.22 generate plots (seaborn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B.23 scale the features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B.24 partion data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part C: Model Evaluation\n",
    "# C.25 model selection via hyper-parameter tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C.26 evaluate model on test data & report accuracy & confusion matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C.27 implement early stop in \"fit\" method for softmax regression model (bonus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C.28 implement stochastic gradient descent logistic regression algorithm (bonus)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
